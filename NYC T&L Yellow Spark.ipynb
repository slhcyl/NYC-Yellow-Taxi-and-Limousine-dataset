{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from azureml.opendatasets import NycTlcGreen\n",
    "from functools import reduce \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import mean, median\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.6,com.microsoft.azure:azure-storage:8.6.6\").getOrCreate()\n",
    "if (spark.getActiveSession()):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"yellow\"\n",
    "blob_sas_token = \"r\"\n",
    "\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "\n",
    "if (spark.getActiveSession()):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "# read parquet, note that it won't load any data yet by now\n",
    "taxi_df = spark.read.parquet(wasbs_path)\n",
    "print('taxi_df is created')\n",
    "\n",
    "# Extract year and month from the pickup_datetime column\n",
    "taxi_df = taxi_df.withColumn(\"year\", F.year(F.col(\"tpepPickupDateTime\")))\n",
    "taxi_df = taxi_df.withColumn(\"month\", F.month(F.col(\"tpepPickupDateTime\")))\n",
    "# Impute missing values with 0\n",
    "taxi_df = taxi_df.fillna(0, subset=[\"fareAmount\"])\n",
    "\n",
    "# Map values to the correct ones\n",
    "taxi_df = taxi_df.withColumn(\"paymentType\", F.when(F.col(\"paymentType\").isin(['Credit','CREDIT','CRD','CRE','Cre', '1']), \"Credit Card\")\\\n",
    "                                      .when(F.col(\"paymentType\").isin(['CAS','CASH','CSH', 'Cash','Cas','2']), \"Cash\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['No Charge','NOC','No', '3']), \"No Charge\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Dispute','DIS', 'Dis','4']), \"Dispute\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Unknown','UNK','NA', '5']), \"Unknown\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Voided trip', '6']), \"Voided trip\")\\\n",
    "                                       .when(F.col('paymentType').contains('No'), 'No Charge')\\\n",
    "                                       .when(F.col('paymentType').rlike('40.|0|NA'), 'Unknown')\n",
    "                          )\n",
    "\n",
    "\n",
    "\n",
    "# Perform aggregation\n",
    "result_df = taxi_df.groupBy(\"paymentType\", \"year\", \"month\") \\\n",
    "            .agg(mean(\"fareAmount\").alias(\"mean_costAmount\"),\n",
    "                 median(\"fareAmount\").alias(\"median_costAmount\"),\n",
    "                 mean(\"totalAmount\").alias(\"mean_priceAmount\"),\n",
    "                 median(\"totalAmount\").alias(\"median_priceAmount\"),\n",
    "                 mean(\"passengerCount\").alias(\"mean_passengerCount\"),\n",
    "                 median(\"passengerCount\").alias(\"median_passengerCount\")) \n",
    "    \n",
    "    # Write each chunk as a separate Parquet file or partition\n",
    "result_df.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(\"NYC_T&L_Yellow\")\n",
    "print('result_df parquet files are created')\n",
    "\n",
    "# stop spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
