{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from azureml.opendatasets import NycTlcGreen\n",
    "from functools import reduce \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import mean, median\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.6,com.microsoft.azure:azure-storage:8.6.6\").getOrCreate()\n",
    "if (spark.getActiveSession()):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"yellow\"\n",
    "blob_sas_token = \"r\"\n",
    "\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "\n",
    "if (spark.getActiveSession()):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "# read parquet, note that it won't load any data yet by now\n",
    "taxi_df = spark.read.parquet(wasbs_path)\n",
    "print('taxi_df is created')\n",
    "\n",
    "# Extract year and month from the pickup_datetime column\n",
    "taxi_df = taxi_df.withColumn(\"year\", F.year(F.col(\"tpepPickupDateTime\")))\n",
    "taxi_df = taxi_df.withColumn(\"month\", F.month(F.col(\"tpepPickupDateTime\")))\n",
    "# Impute missing values with 0\n",
    "taxi_df = taxi_df.fillna(0, subset=[\"fareAmount\"])\n",
    "\n",
    "# Map values to the correct ones\n",
    "taxi_df = taxi_df.withColumn(\"paymentType\", F.when(F.col(\"paymentType\").isin(['Credit','CREDIT','CRD','CRE','Cre', '1']), \"Credit Card\")\\\n",
    "                                      .when(F.col(\"paymentType\").isin(['CAS','CASH','CSH', 'Cash','Cas','2']), \"Cash\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['No Charge','NOC','No', '3']), \"No Charge\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Dispute','DIS', 'Dis','4']), \"Dispute\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Unknown','UNK','NA', '5']), \"Unknown\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Voided trip', '6']), \"Voided trip\")\\\n",
    "                                       .when(F.col('paymentType').contains('No'), 'No Charge')\\\n",
    "                                       .when(F.col('paymentType').rlike('40.|0|NA'), 'Unknown')\n",
    "                          )\n",
    "\n",
    "\n",
    "\n",
    "# Perform aggregation\n",
    "result_df = taxi_df.groupBy(\"paymentType\", \"year\", \"month\") \\\n",
    "            .agg(mean(\"fareAmount\").alias(\"mean_costAmount\"),\n",
    "                 median(\"fareAmount\").alias(\"median_costAmount\"),\n",
    "                 mean(\"totalAmount\").alias(\"mean_priceAmount\"),\n",
    "                 median(\"totalAmount\").alias(\"median_priceAmount\"),\n",
    "                 mean(\"passengerCount\").alias(\"mean_passengerCount\"),\n",
    "                 median(\"passengerCount\").alias(\"median_passengerCount\")) \n",
    "    \n",
    "    # Write each chunk as a separate Parquet file or partition\n",
    "result_df.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(\"NYC_T&L_Yellow\")\n",
    "print('result_df parquet files are created')\n",
    "\n",
    "# stop spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run:\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from azureml.opendatasets import NycTlcGreen\n",
    "from functools import reduce \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import mean, median\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.6,com.microsoft.azure:azure-storage:8.6.6\").getOrCreate()\n",
    "if (spark.getActiveSession()):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "\n",
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"yellow\"\n",
    "blob_sas_token = \"r\"\n",
    "\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "\n",
    "if (spark.getActiveSession()):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "# read parquet\n",
    "taxi_df = spark.read.parquet(wasbs_path)\n",
    "print('taxi_df is created')\n",
    "# df.createOrReplaceTempView('source')\n",
    "limited_df = taxi_df.limit(5)\n",
    "print('limited_df is created')\n",
    "\n",
    "# Extract year and month from the pickup_datetime column\n",
    "limited_df = limited_df.withColumn(\"year\", F.year(F.col(\"tpepPickupDateTime\")))\n",
    "limited_df = limited_df.withColumn(\"month\", F.month(F.col(\"tpepPickupDateTime\")))\n",
    "# Impute missing values with 0\n",
    "limited_df = limited_df.fillna(0, subset=[\"fareAmount\"])\n",
    "\n",
    "# Map values to the correct ones\n",
    "limited_df = limited_df.withColumn(\"paymentType\", F.when(F.col(\"paymentType\").isin(['Credit','CREDIT','CRD','CRE','Cre', '1']), \"Credit Card\")\\\n",
    "                                      .when(F.col(\"paymentType\").isin(['CAS','CASH','CSH', 'Cash','Cas','2']), \"Cash\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['No Charge','NOC','No', '3']), \"No Charge\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Dispute','DIS', 'Dis','4']), \"Dispute\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Unknown','UNK','NA', '5']), \"Unknown\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Voided trip', '6']), \"Voided trip\")\\\n",
    "                                       .when(F.col('paymentType').contains('No'), 'No Charge')\\\n",
    "                                       .when(F.col('paymentType').rlike('40.|0|NA'), 'Unknown')\n",
    "                          )\n",
    "                                      \n",
    "print('cleaninng data is completed')\n",
    "\n",
    "# Perform Aggregation\n",
    "result_df = limited_df.groupBy(\"paymentType\", \"year\", \"month\") \\\n",
    "            .agg(F.mean(\"fareAmount\").alias(\"mean_costAmount\"),\n",
    "                 F.median(\"fareAmount\").alias(\"median_costAmount\"),\n",
    "                 F.mean(\"totalAmount\").alias(\"mean_priceAmount\"),\n",
    "                 F.median(\"totalAmount\").alias(\"median_priceAmount\"),\n",
    "                 F.mean(\"passengerCount\").alias(\"mean_passengerCount\"),\n",
    "                 F.median(\"passengerCount\").alias(\"median_passengerCount\"))\n",
    "# Generate output to folder\n",
    "result_df.write.parquet('sample')\n",
    "print('result is complete')\n",
    "\n",
    "# check file data\n",
    "spark.read.parquet('part-00000-13657153-c709-4034-a595-0bb7391af309-c000.snappy.parquet').show()\n",
    "\n",
    "# stop session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paymentType</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>mean_costAmount</th>\n",
       "      <th>median_costAmount</th>\n",
       "      <th>mean_priceAmount</th>\n",
       "      <th>median_priceAmount</th>\n",
       "      <th>mean_passengerCount</th>\n",
       "      <th>median_passengerCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cash</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.50</td>\n",
       "      <td>10.50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>18.75</td>\n",
       "      <td>18.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paymentType  year  month  mean_costAmount  median_costAmount  \\\n",
       "0         Cash  2012      2              9.5                9.5   \n",
       "1  Credit Card  2012      3             15.3               15.3   \n",
       "2  Credit Card  2012      2             12.5               12.5   \n",
       "\n",
       "   mean_priceAmount  median_priceAmount  mean_passengerCount  \\\n",
       "0             10.50               10.50                  2.0   \n",
       "1             18.75               18.75                  1.0   \n",
       "2             15.00               15.00                  1.0   \n",
       "\n",
       "   median_passengerCount  \n",
       "0                    2.0  \n",
       "1                    1.0  \n",
       "2                    1.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'path_to_file.parquet' with the actual file path\n",
    "df = pd.read_parquet('part-00000-13657153-c709-4034-a595-0bb7391af309-c000.snappy.parquet')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
