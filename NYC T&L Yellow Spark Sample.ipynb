{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset contains historical records accumulated from 2009 to 2018, however, only random 5 records to be performed aggregation.\n",
    "# Spark Dataframe:\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from azureml.opendatasets import NycTlcGreen\n",
    "from functools import reduce \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import mean, median\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.6,com.microsoft.azure:azure-storage:8.6.6\").getOrCreate()\n",
    "if (spark.getActiveSession()):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "\n",
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"yellow\"\n",
    "blob_sas_token = \"r\"\n",
    "\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "\n",
    "if (spark.getActiveSession()):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "# read parquet\n",
    "taxi_df = spark.read.parquet(wasbs_path)\n",
    "print('taxi_df is created')\n",
    "\n",
    "# Sample 5 records\n",
    "limited_df = taxi_df.limit(5)\n",
    "print('limited_df is created')\n",
    "\n",
    "# Extract year and month from the pickup_datetime column\n",
    "limited_df = limited_df.withColumn(\"year\", F.year(F.col(\"tpepPickupDateTime\")))\n",
    "limited_df = limited_df.withColumn(\"month\", F.month(F.col(\"tpepPickupDateTime\")))\n",
    "# Impute missing values with 0\n",
    "limited_df = limited_df.fillna(0, subset=[\"fareAmount\"])\n",
    "\n",
    "# Map values to the correct ones\n",
    "limited_df = limited_df.withColumn(\"paymentType\", F.when(F.col(\"paymentType\").isin(['Credit','CREDIT','CRD','CRE','Cre', '1']), \"Credit Card\")\\\n",
    "                                      .when(F.col(\"paymentType\").isin(['CAS','CASH','CSH', 'Cash','Cas','2']), \"Cash\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['No Charge','NOC','No', '3']), \"No Charge\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Dispute','DIS', 'Dis','4']), \"Dispute\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Unknown','UNK','NA', '5']), \"Unknown\")\\\n",
    "                                       .when(F.col(\"paymentType\").isin(['Voided trip', '6']), \"Voided trip\")\\\n",
    "                                       .when(F.col('paymentType').contains('No'), 'No Charge')\\\n",
    "                                       .when(F.col('paymentType').rlike('40.|0|NA'), 'Unknown')\n",
    "                          )\n",
    "                                      \n",
    "print('cleaninng data is completed.')\n",
    "\n",
    "# Perform Aggregation\n",
    "result_df = limited_df.groupBy(\"paymentType\", \"year\", \"month\") \\\n",
    "            .agg(F.mean(\"fareAmount\").alias(\"mean_costAmount\"),\n",
    "                 F.median(\"fareAmount\").alias(\"median_costAmount\"),\n",
    "                 F.mean(\"totalAmount\").alias(\"mean_priceAmount\"),\n",
    "                 F.median(\"totalAmount\").alias(\"median_priceAmount\"),\n",
    "                 F.mean(\"passengerCount\").alias(\"mean_passengerCount\"),\n",
    "                 F.median(\"passengerCount\").alias(\"median_passengerCount\"))\n",
    "\n",
    "# Output data frame to parquet file\n",
    "result_df.write.parquet('sample')\n",
    "print('the job is complete.')\n",
    "\n",
    "# check output data\n",
    "spark.read.parquet('part-00000-13657153-c709-4034-a595-0bb7391af309-c000.snappy.parquet').show()\n",
    "\n",
    "# stop session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
